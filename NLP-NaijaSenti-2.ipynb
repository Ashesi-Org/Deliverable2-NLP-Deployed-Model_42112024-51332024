{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haliu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haliu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINING HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameters\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = 3\n",
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = 20000\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "# define the device to use\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train datasets for the three languages\n",
    "hausa_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/Project_Languages/hausa/train.tsv\",sep='\\t')\n",
    "igbo_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/Project_Languages/igbo/train.tsv\",sep='\\t')\n",
    "pidgin_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/Project_Languages/pidgin/train.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the three dataframes based on the common columns 'tweet' and 'label'\n",
    "merged_df = pd.concat([hausa_df, igbo_df, pidgin_df], axis=0)\n",
    "\n",
    "# Save the merged dataset to a new file\n",
    "merged_df.to_csv('merged_dataset.tsv', index=False)\n",
    "\n",
    "# Import the merged dataset\n",
    "merged_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/merged_dataset.tsv\", sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the validate datasets for the three languages\n",
    "hausa_validate_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/Project_Languages/hausa/dev.tsv\",sep='\\t')\n",
    "igbo_validate_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/Project_Languages/igbo/dev.tsv\",sep='\\t')\n",
    "pidgin_validate_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/Project_Languages/pidgin/dev.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the three dataframes based on the common columns 'tweet' and 'label'\n",
    "merged_validate_df = pd.concat([hausa_validate_df, igbo_validate_df, pidgin_validate_df], axis=0)\n",
    "\n",
    "# Save the merged dataset to a new file\n",
    "merged_validate_df.to_csv('merged_validate_dataset.tsv', index=False)\n",
    "\n",
    "# Import the merged dataset\n",
    "merged_validate_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/merged_validate_dataset.tsv\", sep=',')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the tweets\n",
    "def clean_tweet(tweet):\n",
    "    # Convert the tweet to lowercase\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Tokenize the tweet\n",
    "    words = word_tokenize(tweet)\n",
    "\n",
    "    # Remove non-alphanumeric characters\n",
    "    words = [re.sub(r'\\W+', '', word) for word in words]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    stop_words += ['user', 'im', 'una', 'na', 'wer', 'dey', 'us', 'dem', 'dat', 'omo', 'wey']\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Remove empty strings and single characters\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "\n",
    "    # Join the words back into a string\n",
    "    cleaned_tweet = ' '.join(words)\n",
    "\n",
    "    return cleaned_tweet\n",
    "\n",
    "# Apply the clean_tweet function to the \"tweet\" column of the dataframe\n",
    "merged_df[\"tweet\"] = merged_df[\"tweet\"].apply(clean_tweet)\n",
    "\n",
    "merged_validate_df[\"tweet\"] = merged_validate_df[\"tweet\"].apply(clean_tweet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERFORMING STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the SnowballStemmer class for each language\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# function to perform stemming on a text based on its language\n",
    "def stem_text(tweet):\n",
    "    return \" \".join([stemmer.stem(word) for word in nltk.word_tokenize(tweet)])\n",
    "    \n",
    "# apply the stem_text function to the \"tweet\" column of the DataFrame\n",
    "merged_df[\"tweet\"] = merged_df[\"tweet\"].apply(stem_text)\n",
    "\n",
    "# apply the stem_text function to the \"tweet\" column of the DataFrame\n",
    "merged_validate_df[\"tweet\"] = merged_validate_df[\"tweet\"].apply(stem_text)\n",
    "\n",
    "# Save the cleaned dataframe to a new csv file\n",
    "merged_df.to_csv(\"cleaned_tweets.tsv\", index=False)\n",
    "\n",
    "# Save the cleaned dataframe to a new csv file\n",
    "merged_validate_df.to_csv(\"cleaned_tweets_validate.tsv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the cleaned datasets\n",
    "merged_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/cleaned_tweets.tsv\", sep=',')\n",
    "\n",
    "merged_validate_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/cleaned_tweets_validate.tsv\", sep=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENCODING THE LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the labels into int type\n",
    "label_encoder = LabelEncoder()\n",
    "merged_df['label'] = label_encoder.fit_transform(merged_df['label'])\n",
    "\n",
    "# Save the encoded dataframe to a new csv file\n",
    "merged_df.to_csv(\"merged_encoded_dataset.tsv\", index=False)\n",
    "\n",
    "merged_validate_df['label'] = label_encoder.fit_transform(merged_validate_df['label'])\n",
    "\n",
    "# Save the encoded dataframe to a new csv file\n",
    "merged_validate_df.to_csv(\"merged_encoded_dataset_validate.tsv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allah ya kai rahmarsa kabarin ta uwa ga gimbiy...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wannan kasa tamu allah ya kyauta wai lailai sa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nan da zuwa shekara mai zuwa da yarda allah</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hhm rahama allah ya shiry ki dinga daukan shaw...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>innalillahi wainnailaihirrajiun allah swa ya k...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  allah ya kai rahmarsa kabarin ta uwa ga gimbiy...      2\n",
       "1  wannan kasa tamu allah ya kyauta wai lailai sa...      2\n",
       "2        nan da zuwa shekara mai zuwa da yarda allah      2\n",
       "3  hhm rahama allah ya shiry ki dinga daukan shaw...      2\n",
       "4  innalillahi wainnailaihirrajiun allah swa ya k...      2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the encoded datasets\n",
    "dasaset_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/merged_encoded_dataset.tsv\")\n",
    "dasaset_validate_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/merged_encoded_dataset_validate.tsv\")\n",
    "dasaset_validate_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING TEST DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hausa_test_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/Project_Languages/hausa/test.tsv\",sep='\\t')\n",
    "igbo_test_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/Project_Languages/igbo/test.tsv\",sep='\\t')\n",
    "pidgin_test_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/Project_Languages/pidgin/test.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the labels into int type\n",
    "hausa_test_df['label'] = label_encoder.fit_transform(hausa_test_df['label'])\n",
    "igbo_test_df['label'] = label_encoder.fit_transform(igbo_test_df['label'])\n",
    "pidgin_test_df['label'] = label_encoder.fit_transform(pidgin_test_df['label'])\n",
    "\n",
    "# Save the encoded dataframe to a new csv file\n",
    "hausa_test_df.to_csv(\"hausa_dataset.tsv\", index=False)\n",
    "igbo_test_df.to_csv(\"igbo_dataset.tsv\", index=False)\n",
    "pidgin_test_df.to_csv(\"pidgin_dataset.tsv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets\n",
    "hausa_test_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/hausa_dataset.tsv\", sep=',')\n",
    "igbo_test_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/igbo_dataset.tsv\", sep=',')\n",
    "hausa_test_df = pd.read_csv(\"C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/pidgin_dataset.tsv\", sep=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE DATASET WITH TORCHTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fields\n",
    "TEXT = torchtext.legacy.data.Field(\n",
    "    tokenize='spacy',\n",
    "    sequential=True,\n",
    "    tokenizer_language='en_core_web_sm'\n",
    ")\n",
    "LABEL = torchtext.legacy.data.LabelField(dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files\n",
    "train_df = pd.read_csv('C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/merged_encoded_dataset.tsv', usecols=[0, 1], names=['tweet', 'label'], header=None)\n",
    "validate_df = pd.read_csv('C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/merged_encoded_dataset_validate.tsv', usecols=[0, 1], names=['tweet', 'label'], header=None)\n",
    "\n",
    "hausa_test_df = pd.read_csv('C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/hausa_dataset.tsv', usecols=[0, 1], names=['tweet', 'label'], header=None)\n",
    "igbo_test_df = pd.read_csv('C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/igbo_dataset.tsv', usecols=[0, 1], names=['tweet', 'label'], header=None)\n",
    "pidgin_test_df = pd.read_csv('C:/Users/haliu/OneDrive/Desktop/CONTAINER/JUNIOR YEAR/SEM 1/NLP/pidgin_dataset.tsv', usecols=[0, 1], names=['tweet', 'label'], header=None)\n",
    "\n",
    "train_df = train_df.drop(labels=0, axis=0)\n",
    "validate_df = validate_df.drop(labels=0, axis=0)\n",
    "\n",
    "hausa_test_df = hausa_test_df.drop(labels=0, axis=0)\n",
    "igbo_test_df = igbo_test_df.drop(labels=0, axis=0)\n",
    "pidgin_test_df = pidgin_test_df.drop(labels=0, axis=0)\n",
    "\n",
    "# Create the examples\n",
    "examples = [torchtext.legacy.data.Example.fromlist([row['tweet'], row['label']], fields=[('tweet', TEXT), ('label', LABEL)]) for _, row in train_df.iterrows()]\n",
    "validate_examples = [torchtext.legacy.data.Example.fromlist([row['tweet'], row['label']], fields=[('tweet', TEXT), ('label', LABEL)]) for _, row in validate_df.iterrows()]\n",
    "\n",
    "hausa_examples = [torchtext.legacy.data.Example.fromlist([row['tweet'], row['label']], fields=[('tweet', TEXT), ('label', LABEL)]) for _, row in hausa_test_df.iterrows()]\n",
    "igbo_examples = [torchtext.legacy.data.Example.fromlist([row['tweet'], row['label']], fields=[('tweet', TEXT), ('label', LABEL)]) for _, row in igbo_test_df.iterrows()]\n",
    "pidgin_examples = [torchtext.legacy.data.Example.fromlist([row['tweet'], row['label']], fields=[('tweet', TEXT), ('label', LABEL)]) for _, row in pidgin_test_df.iterrows()]\n",
    "\n",
    "# Create the dataset\n",
    "dataset = torchtext.legacy.data.Dataset(examples, fields=[('tweet', TEXT), ('label', LABEL)])\n",
    "validate_dataset = torchtext.legacy.data.Dataset(validate_examples, fields=[('tweet', TEXT), ('label', LABEL)])\n",
    "\n",
    "\n",
    "hausa_test_set = torchtext.legacy.data.Dataset(hausa_examples, fields=[('tweet', TEXT), ('label', LABEL)])\n",
    "igbo_test_set = torchtext.legacy.data.Dataset(igbo_examples, fields=[('tweet', TEXT), ('label', LABEL)])\n",
    "pidgin_test_set = torchtext.legacy.data.Dataset(pidgin_examples, fields=[('tweet', TEXT), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 29485\n",
      "Num Test: 5303\n",
      "Num Test: 3682\n",
      "Num Test: 4154\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset\n",
    "valid_data = validate_dataset\n",
    "hausa_test_data = hausa_test_set\n",
    "igbo_test_data = igbo_test_set\n",
    "pidgin_test_data = pidgin_test_set\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Test: {len(hausa_test_data)}')\n",
    "print(f'Num Test: {len(igbo_test_data)}')\n",
    "print(f'Num Test: {len(pidgin_test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 29485\n",
      "Num Validation: 5799\n"
     ]
    }
   ],
   "source": [
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Validation: {len(valid_data)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vocabulary based on the top \"VOCABULARY_SIZE\" words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20002\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(dataset, max_size=VOCABULARY_SIZE)\n",
    "LABEL.build_vocab(dataset)\n",
    "\n",
    "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "print(f'Number of classes: {len(LABEL.vocab)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, hausa_test_loader, igbo_test_loader, pidgin_test_loader  = \\\n",
    "    torchtext.legacy.data.BucketIterator.splits(\n",
    "        (train_data, valid_data, hausa_test_data, igbo_test_data, pidgin_test_data),\n",
    "         batch_size=BATCH_SIZE,\n",
    "         sort_within_batch=False,\n",
    "         sort_key=lambda x: len(x.tweet),\n",
    "         shuffle=True,\n",
    "         device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Text matrix size: torch.Size([48, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "\n",
      "Valid:\n",
      "Text matrix size: torch.Size([3, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "\n",
      "Test:\n",
      "Text matrix size: torch.Size([3, 128])\n",
      "Target vector size: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "for batch in train_loader:\n",
    "    print(f'Text matrix size: {batch.tweet.size()}')\n",
    "    print(f'Target vector size: {batch.label.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nValid:')\n",
    "for batch in valid_loader:\n",
    "    print(f'Text matrix size: {batch.tweet.size()}')\n",
    "    print(f'Target vector size: {batch.label.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nTest:')\n",
    "for batch in hausa_test_loader:\n",
    "    print(f'Text matrix size: {batch.tweet.size()}')\n",
    "    print(f'Target vector size: {batch.label.size()}')\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILDING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # The Embedding layer takes the input text and maps each \n",
    "        # token to its corresponding embedding vector of size \n",
    "        # embedding_dim.\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        # The LSTM layer takes the embedded input and processes \n",
    "        # it through an LSTM neural network with hidden_dim hidden\n",
    "        # units.\n",
    "        self.cnn = nn.LSTM(embedding_dim, hidden_dim) \n",
    "        # Finally, the Linear layer maps the LSTM's output to a \n",
    "        # vector of size output_dim       \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.cnn(embedded)\n",
    "        hidden.squeeze_(0)\n",
    "        output = self.fc(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = SentimentModel(input_dim=len(TEXT.vocab),\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            output_dim=NUM_CLASSES \n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATING MODEL ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(data_loader):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/231 | Loss: 1.1185\n",
      "Epoch: 001/015 | Batch 050/231 | Loss: 1.0928\n",
      "Epoch: 001/015 | Batch 100/231 | Loss: 1.0919\n",
      "Epoch: 001/015 | Batch 150/231 | Loss: 1.1002\n",
      "Epoch: 001/015 | Batch 200/231 | Loss: 1.0917\n",
      "training accuracy: 35.27%\n",
      "valid accuracy: 35.89%\n",
      "Time elapsed: 2.40 min\n",
      "Epoch: 002/015 | Batch 000/231 | Loss: 1.0953\n",
      "Epoch: 002/015 | Batch 050/231 | Loss: 1.1051\n",
      "Epoch: 002/015 | Batch 100/231 | Loss: 1.0984\n",
      "Epoch: 002/015 | Batch 150/231 | Loss: 1.0331\n",
      "Epoch: 002/015 | Batch 200/231 | Loss: 0.8676\n",
      "training accuracy: 58.59%\n",
      "valid accuracy: 50.13%\n",
      "Time elapsed: 4.65 min\n",
      "Epoch: 003/015 | Batch 000/231 | Loss: 0.8047\n",
      "Epoch: 003/015 | Batch 050/231 | Loss: 0.8184\n",
      "Epoch: 003/015 | Batch 100/231 | Loss: 0.7416\n",
      "Epoch: 003/015 | Batch 150/231 | Loss: 0.8100\n",
      "Epoch: 003/015 | Batch 200/231 | Loss: 0.6477\n",
      "training accuracy: 77.36%\n",
      "valid accuracy: 62.15%\n",
      "Time elapsed: 6.90 min\n",
      "Epoch: 004/015 | Batch 000/231 | Loss: 0.6238\n",
      "Epoch: 004/015 | Batch 050/231 | Loss: 0.6383\n",
      "Epoch: 004/015 | Batch 100/231 | Loss: 0.6051\n",
      "Epoch: 004/015 | Batch 150/231 | Loss: 0.5254\n",
      "Epoch: 004/015 | Batch 200/231 | Loss: 0.5774\n",
      "training accuracy: 85.08%\n",
      "valid accuracy: 65.86%\n",
      "Time elapsed: 9.01 min\n",
      "Epoch: 005/015 | Batch 000/231 | Loss: 0.4282\n",
      "Epoch: 005/015 | Batch 050/231 | Loss: 0.5344\n",
      "Epoch: 005/015 | Batch 100/231 | Loss: 0.3506\n",
      "Epoch: 005/015 | Batch 150/231 | Loss: 0.4419\n",
      "Epoch: 005/015 | Batch 200/231 | Loss: 0.5288\n",
      "training accuracy: 89.89%\n",
      "valid accuracy: 67.75%\n",
      "Time elapsed: 11.13 min\n",
      "Epoch: 006/015 | Batch 000/231 | Loss: 0.2531\n",
      "Epoch: 006/015 | Batch 050/231 | Loss: 0.2127\n",
      "Epoch: 006/015 | Batch 100/231 | Loss: 0.3951\n",
      "Epoch: 006/015 | Batch 150/231 | Loss: 0.3600\n",
      "Epoch: 006/015 | Batch 200/231 | Loss: 0.3472\n",
      "training accuracy: 93.21%\n",
      "valid accuracy: 68.79%\n",
      "Time elapsed: 13.15 min\n",
      "Epoch: 007/015 | Batch 000/231 | Loss: 0.1496\n",
      "Epoch: 007/015 | Batch 050/231 | Loss: 0.2335\n",
      "Epoch: 007/015 | Batch 100/231 | Loss: 0.2288\n",
      "Epoch: 007/015 | Batch 150/231 | Loss: 0.2071\n",
      "Epoch: 007/015 | Batch 200/231 | Loss: 0.2938\n",
      "training accuracy: 94.86%\n",
      "valid accuracy: 68.60%\n",
      "Time elapsed: 15.25 min\n",
      "Epoch: 008/015 | Batch 000/231 | Loss: 0.1131\n",
      "Epoch: 008/015 | Batch 050/231 | Loss: 0.1346\n",
      "Epoch: 008/015 | Batch 100/231 | Loss: 0.2863\n",
      "Epoch: 008/015 | Batch 150/231 | Loss: 0.1486\n",
      "Epoch: 008/015 | Batch 200/231 | Loss: 0.2063\n",
      "training accuracy: 95.76%\n",
      "valid accuracy: 67.65%\n",
      "Time elapsed: 17.31 min\n",
      "Epoch: 009/015 | Batch 000/231 | Loss: 0.1638\n",
      "Epoch: 009/015 | Batch 050/231 | Loss: 0.1372\n",
      "Epoch: 009/015 | Batch 100/231 | Loss: 0.1671\n",
      "Epoch: 009/015 | Batch 150/231 | Loss: 0.2344\n",
      "Epoch: 009/015 | Batch 200/231 | Loss: 0.1388\n",
      "training accuracy: 96.93%\n",
      "valid accuracy: 66.61%\n",
      "Time elapsed: 19.20 min\n",
      "Epoch: 010/015 | Batch 000/231 | Loss: 0.1574\n",
      "Epoch: 010/015 | Batch 050/231 | Loss: 0.0569\n",
      "Epoch: 010/015 | Batch 100/231 | Loss: 0.1380\n",
      "Epoch: 010/015 | Batch 150/231 | Loss: 0.1433\n",
      "Epoch: 010/015 | Batch 200/231 | Loss: 0.1478\n",
      "training accuracy: 97.39%\n",
      "valid accuracy: 66.65%\n",
      "Time elapsed: 21.07 min\n",
      "Epoch: 011/015 | Batch 000/231 | Loss: 0.1061\n",
      "Epoch: 011/015 | Batch 050/231 | Loss: 0.0646\n",
      "Epoch: 011/015 | Batch 100/231 | Loss: 0.0863\n",
      "Epoch: 011/015 | Batch 150/231 | Loss: 0.1253\n",
      "Epoch: 011/015 | Batch 200/231 | Loss: 0.1142\n",
      "training accuracy: 97.37%\n",
      "valid accuracy: 66.05%\n",
      "Time elapsed: 22.93 min\n",
      "Epoch: 012/015 | Batch 000/231 | Loss: 0.0394\n",
      "Epoch: 012/015 | Batch 050/231 | Loss: 0.1201\n",
      "Epoch: 012/015 | Batch 100/231 | Loss: 0.0959\n",
      "Epoch: 012/015 | Batch 150/231 | Loss: 0.1759\n",
      "Epoch: 012/015 | Batch 200/231 | Loss: 0.1310\n",
      "training accuracy: 97.82%\n",
      "valid accuracy: 67.94%\n",
      "Time elapsed: 24.80 min\n",
      "Epoch: 013/015 | Batch 000/231 | Loss: 0.0346\n",
      "Epoch: 013/015 | Batch 050/231 | Loss: 0.1236\n",
      "Epoch: 013/015 | Batch 100/231 | Loss: 0.0917\n",
      "Epoch: 013/015 | Batch 150/231 | Loss: 0.0671\n",
      "Epoch: 013/015 | Batch 200/231 | Loss: 0.1222\n",
      "training accuracy: 97.94%\n",
      "valid accuracy: 66.61%\n",
      "Time elapsed: 26.73 min\n",
      "Epoch: 014/015 | Batch 000/231 | Loss: 0.0330\n",
      "Epoch: 014/015 | Batch 050/231 | Loss: 0.0342\n",
      "Epoch: 014/015 | Batch 100/231 | Loss: 0.0633\n",
      "Epoch: 014/015 | Batch 150/231 | Loss: 0.0872\n",
      "Epoch: 014/015 | Batch 200/231 | Loss: 0.1430\n",
      "training accuracy: 98.33%\n",
      "valid accuracy: 66.86%\n",
      "Time elapsed: 28.54 min\n",
      "Epoch: 015/015 | Batch 000/231 | Loss: 0.0358\n",
      "Epoch: 015/015 | Batch 050/231 | Loss: 0.0543\n",
      "Epoch: 015/015 | Batch 100/231 | Loss: 0.1355\n",
      "Epoch: 015/015 | Batch 150/231 | Loss: 0.1208\n",
      "Epoch: 015/015 | Batch 200/231 | Loss: 0.1411\n",
      "training accuracy: 98.43%\n",
      "valid accuracy: 66.30%\n",
      "Time elapsed: 30.43 min\n",
      "Total Training Time: 30.44 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "NUM_EPOCHS = 15\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Train the model\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (tweets, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        tweets = tweets.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(tweets)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        if not i % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {i:03d}/{len(train_loader):03d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "            \n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(train_loader):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(valid_loader):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATING THE MODEL ON THE TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Hausa: 64.55%\n",
      "Test accuracy for Igbo: 66.51%\n",
      "Test accuracy for Pidgin: 55.42%\n"
     ]
    }
   ],
   "source": [
    "print(f'Test accuracy for Hausa: {compute_accuracy(hausa_test_loader):.2f}%')\n",
    "print(f'Test accuracy for Igbo: {compute_accuracy(igbo_test_loader):.2f}%')\n",
    "print(f'Test accuracy for Pidgin: {compute_accuracy(pidgin_test_loader):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the testing function\n",
    "def test(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            test_acc += torch.sum(preds == labels)\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc /= len(dataloader.dataset)\n",
    "\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8648530512020506, tensor(0.6651))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, criterion, igbo_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            # Forward pass\n",
    "            batch_y_pred = model(batch_x)\n",
    "\n",
    "            # Get predicted labels\n",
    "            _, batch_y_pred = torch.max(batch_y_pred, dim=1)\n",
    "\n",
    "            # Append true and predicted labels\n",
    "            y_true += batch_y.tolist()\n",
    "            y_pred += batch_y_pred.tolist()\n",
    "\n",
    "    # Calculate precision, recall, and F-1 score\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "    print(\"Precision: {:.2f}\".format(precision))\n",
    "    print(\"Recall: {:.2f}\".format(recall))\n",
    "    print(\"F-1 score: {:.2f}\".format(f1_score))\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.64\n",
      "Recall: 0.65\n",
      "F-1 score: 0.64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6432796429289854, 0.6461433602043055, 0.6443836276693253)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, hausa_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.66\n",
      "Recall: 0.65\n",
      "F-1 score: 0.66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6644768418248478, 0.650734207688415, 0.6562284559719662)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, igbo_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.44\n",
      "Recall: 0.43\n",
      "F-1 score: 0.43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4414327903540962, 0.4305022503608344, 0.43088680864047846)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, pidgin_test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING FOR HAUSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: rahama yan mata\n",
      "Original label: 2\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: saboda corona wai\n",
      "Original label: 1\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: ubanka zama daram <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: gaisuwa muke babban director\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: yau ba face mask\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: mutumin nan akwai <unk> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: amma kina da kyau fa\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: shi kuma yayi kukan giwa\n",
      "Original label: 1\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: ta <unk> <unk> zaa sawa\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: allah ya tsine mai karya <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: toh fa allah ya ida nufi\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: su <unk> shi <unk> <unk> nijeriya\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: <unk> fah nan kuma suka koma\n",
      "Original label: 1\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: ita da ba wanka take ba <pad>\n",
      "Original label: 0\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: allah ya kara <unk> shall <unk> higher\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: tabb wannan shine abba gida gida din\n",
      "Original label: 1\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: ruwan zafi ba wurin wasan kifi bane <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: hhhhh <unk> nee wannan biki ya <unk> ni\n",
      "Original label: 2\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: <unk> ba garin neman <unk> an nemo rama\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: toh wallahi ayi musu bulala sai sunyi <unk> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: you see <unk> <unk> <unk> yinki har a snapchat\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: wasu yan takarar pdp <unk> <unk> hoto da jonathan <pad>\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: <unk> wonder kike da kyau ke yar barca ce shiyasa\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: gari yayi <unk> sunan daya dace da wann tafiyar kenan\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: bashida wani tasiri a nan domin basusan darajar abin ba <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: <unk> rahama antashi <unk> yunwa nakeji ataimaka abani abunda <unk> <unk> <pad>\n",
      "Original label: 1\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: allah ubangiji ya kawo mana zaman lafiya mai dorewa akasarmu nigeria ameen\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: wlh saidai <unk> dayayi <unk> <unk> <unk> wlh shi <unk> baxasu iya <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: nifa banga naki <unk> yaki ba ko basu fara raba muku bindiga ba <pad>\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: <unk> allah ya <unk> a duk inda yake <unk> <unk> wallah <unk> <unk> clip\n",
      "Original label: 2\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: lallai wann abun haushi ne sulhu d dan taadda <unk> finafinan india baa wann <pad> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: a a mutuniyar fa yau ansa hijab gashi har kinyi kyau dole <unk> <unk> india gun <pad>\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: allah kamana maganin wannan rashin tsaro yunwa da tsadar rayuwa da muke fama da ita a nigeria <pad>\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: eh bamayi bamayi ba wanan <unk> maikyau musamman idan shugaba ya gaza wajen bada <unk> musamman akan alumarsa <pad> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: mun gode mishi fa san <unk> da aiki amma dan allah <unk> zai sauqa ne kam nina <unk> mulkin shi <pad>\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: kuna <unk> kaman saint <unk> duk cin ku ake yayan iska kawai kuje kuyi <unk> yafi ye muku wannan yawon banzan <pad> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: hoo dan banza nide <unk> <unk> inde xaka sake remix din <unk> daga yabon <unk> <unk> idan ka yarda send your account <unk> <pad> <pad>\n",
      "Original label: 1\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: wai ko dai ku <unk> bbc hausa kun ci kuɗin korona ne saboda <unk> ga kullum sai kun fidda sabon billboard <unk> corona new <unk> <pad> <pad> <pad> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: uhmmm allah allah ne lokaci <unk> rana da lokaci sunanan zuwa <unk> kou <unk> <unk> <unk> <unk> <unk> <unk> rayuwarmu <unk> domin addininmu <unk> <unk> addininmu dayardan aubangijin adalci <pad> <pad> <pad> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: gaskiya duk <unk> ya burgeni amma ina ganin ya daga allah kamar ya fi burgeni shi kuma role nata a labarina takaici take bani saboda son da takeyi wa mahmoud yayi yawa wallahi <pad> <pad> <pad> <pad> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: gaskiyan magana shine <unk> abun nata ya fara yawa ya kamata gwamnati ta <unk> mata ko kuma ta taka mata <unk> su iyan gudun <unk> aljanu <unk> su da ba xaa biyo su kasa a basu abincin ba <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: allah yasa tabbatar da hakan idan dai har wannan <unk> ya <unk> kuma babu wane babu <unk> a ciki maana babu talaka babu mai kudi <unk> wallahi muna <unk> da wannan matakin kaucewa hukuncin allah shine karin matsalolin dake addabar alumma a duniya allah yayi maka jagora amin <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Test the model on a sample of the test set\n",
    "for batch in hausa_test_loader:\n",
    "    tweet = batch.tweet\n",
    "    label = batch.label\n",
    "    \n",
    "    # Print the original text and label\n",
    "    text_string = ' '.join([TEXT.vocab.itos[token.item()] for token in tweet[:, 0]])\n",
    "    label_string = LABEL.vocab.itos[label[0].item()]\n",
    "    print('Original tweet:', text_string)\n",
    "    print('Original label:', label_string)\n",
    "    \n",
    "    # Make a prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(tweet)\n",
    "        prediction = torch.argmax(output, dim=1)\n",
    "        prediction_string = LABEL.vocab.itos[prediction[0].item()]\n",
    "        print('Predicted label:', prediction_string)\n",
    "    \n",
    "    print('---------------------------------')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING FOR IGBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: <unk> <unk> oma\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: bia <unk> egwu\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: <unk> onu you <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: nsogbu adiro <unk> m\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: ndi facebook abiala nga\n",
      "Original label: 1\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: hapu <unk> <unk> ahu\n",
      "Original label: 0\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: hate speech gbakwa okwu <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: chris <unk> gba gi ume\n",
      "Original label: 2\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: <unk> gi gara ulo uka\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: bịa buru m nisi <unk> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: ututu oma onyeoma <unk> <unk> <unk>\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: nna biko kowa okwu gị a\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: ara dị m <unk> <unk> <unk> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: oko ogo <unk> diri gi <unk> nma\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: <unk> ozi ogu <unk> husband ozi <unk>\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: obi adiro mu mma <unk> udo di <pad>\n",
      "Original label: 0\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: nwanyi ocha a <unk> m abia ngwa ngwa <pad>\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: <unk> ujo akwukwo ibem mgbe nwa daalu ezigbo mmadu\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: oscar oo kedu ihe anyi gwa gi <unk> onitsha <pad>\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: biko allow <unk> <unk> buga maka <unk> diro <unk> ihe <pad>\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: otua ka <unk> m di agidigba eeee agidigba <unk> <unk> <unk>\n",
      "Original label: 2\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: <unk> <unk> gini <unk> nna <unk> bu <unk> wayo gbawa door <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: <unk> idi <unk> di kwa <unk> gi <unk> <unk> china abu <unk> <pad> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: <unk> nya ihe <unk> chọrọ bụ ka <unk> bịa naa cut m gap okwia <pad>\n",
      "Original label: 1\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: <unk> bu <unk> mmanya ka unu <unk> anwu ka unu <unk> <unk> ka anyi <unk> <pad> <pad> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: igbo bụ igbo ụtụtụ ọma nụ oooo chi a bọọla oooo <unk> gaadịrị anyị <unk> mma naha <unk> <pad> <pad> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: obodo mara mma milk <unk> honey juru obodo oma nke nigeria anambra state oo ihe ala anyi light <unk> <unk> nation <pad> <pad> <pad> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: doctor <unk> afo osisa ji m <unk> olu i <unk> ako nuo nwanyi okwa <unk> di ndu <unk> anu nwanyi udili doctor gini ka <unk> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: nna anyị nke <unk> hụrụ anyị nanya mgbe <unk> <unk> obi <unk> a <unk> bụ naanị ihe anyị <unk> mee ka <unk> gị gaa <unk> nke ụtụtụ a <unk> <unk> <unk> site <unk> ruo ụtụtụ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 1\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the model on a sample of the test set\n",
    "for batch in igbo_test_loader:\n",
    "    tweet = batch.tweet\n",
    "    label = batch.label\n",
    "    \n",
    "    # Print the original text and label\n",
    "    text_string = ' '.join([TEXT.vocab.itos[token.item()] for token in tweet[:, 0]])\n",
    "    label_string = LABEL.vocab.itos[label[0].item()]\n",
    "    print('Original tweet:', text_string)\n",
    "    print('Original label:', label_string)\n",
    "    \n",
    "    # Make a prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(tweet)\n",
    "        prediction = torch.argmax(output, dim=1)\n",
    "        prediction_string = LABEL.vocab.itos[prediction[0].item()]\n",
    "        print('Predicted label:', prediction_string)\n",
    "    \n",
    "    print('---------------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING FOR PIDGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: <unk> be <unk> <pad> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: one pikin wia get <unk> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: make <unk> come see <unk> <unk>\n",
      "Original label: 2\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: <unk> <unk> <unk> even vex <unk> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: wetin <unk> <unk> find <unk> <unk> test <pad>\n",
      "Original label: 1\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: cos <unk> <unk> <unk> you fit do <unk>\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: <unk> weyrey say <unk> group baba <unk> group <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: make <unk> month do finish abeg i <unk> tire <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: your mind <unk> sugar <unk> <unk> man <unk> <unk> win\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: who do <unk> <unk> be <unk> <unk> even give <unk> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: guy <unk> <unk> bring <unk> kind <unk> come <unk> <unk> <unk> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: <unk> <unk> avoid <unk> boy he <unk> even answer <unk> <unk> <unk> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: you <unk> be <unk> want pass <unk> <unk> god say <unk> <unk> field\n",
      "Original label: 2\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: person father die <unk> <unk> <unk> say make <unk> stop <unk> <unk> <unk> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: hail <unk> twitter <unk> <unk> <unk> <unk> <unk> comot <unk> go do <unk> <unk> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: <unk> <unk> <unk> face sing am you think sey she <unk> wan do cover okoto <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: <unk> i come <unk> <unk> <unk> <unk> go <unk> ask weda i <unk> give <unk> free <pad>\n",
      "Original label: 1\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: <unk> cos <unk> psg <unk> <unk> <unk> <unk> <unk> <unk> wen <unk> <unk> win <unk> <unk> wenger <pad>\n",
      "Original label: 2\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: yrs <unk> <unk> <unk> how much i heard <unk> radio say <unk> like <unk> i wan do am <pad>\n",
      "Original label: 2\n",
      "Predicted label: 1\n",
      "---------------------------------\n",
      "Original tweet: apc abeg <unk> joke <unk> <unk> <unk> <unk> <unk> coro never go finish <unk> make <unk> social <unk> abeg <pad>\n",
      "Original label: 2\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: ndi igbo si <unk> voom ka mma karia statement <unk> bro <unk> ur <unk> make papa <unk> <unk> <unk> <unk> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: <unk> <unk> bala bala <unk> <unk> <unk> an <unk> fact <unk> be <unk> get big yansh <unk> <unk> shake <unk> soft <pad>\n",
      "Original label: 2\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: i m <unk> even <unk> <unk> big ass fit kill <unk> <unk> <unk> life like wtf <unk> <unk> i ca nt <unk> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: come you that <unk> <unk> retweet <unk> <unk> <unk> <unk> <unk> <unk> like <unk> <unk> <unk> ashewo <unk> you <unk> <unk> abeg abeg <pad> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: <unk> who get <unk> <unk> <unk> <unk> heat <unk> die abi <unk> <unk> <unk> side <unk> heat <unk> <unk> what kind <unk> sun is <unk> <pad> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: abeg come <unk> buy <unk> i sell <unk> <unk> <unk> <unk> <unk> samsung <unk> i sell all brandskind <unk> laptop abeg <unk> <unk> i m <unk> cac <pad> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: omg i swear <unk> <unk> <unk> drop <unk> <unk> <unk> <unk> i hear say <unk> <unk> african <unk> <unk> <unk> en win <unk> <unk> <unk> is a good <unk> <pad> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: <unk> be today we done <unk> <unk> <unk> fight <unk> <unk> <unk> <unk> <unk> mama born <unk> i done <unk> vex <unk> <unk> <unk> rt <unk> you love <unk> vibe <unk> <pad> <pad> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: i <unk> straight <unk> <unk> heart <unk> <unk> <unk> driver <unk> <unk> song <unk> he said abeg you fit play that your straight <unk> <unk> <unk> song <unk> abeg i <unk> feel that song <pad> <pad> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 2\n",
      "---------------------------------\n",
      "Original tweet: i m a product <unk> grace <unk> i am <unk> have are <unk> <unk> <unk> <unk> god <unk> you ever ask how i made it <unk> i do <unk> i say <unk> god <unk> vex <unk> <unk> <pad> <pad> <pad> <pad> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: abeg i never get <unk> money <unk> day spend abeg nah god name i dy use beg <unk> make <unk> <unk> dy steal <unk> money <unk> <unk> <unk> <unk> finish <unk> i <unk> make call reach hour ehhh wetin i do <unk> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: d way i love dis eeh <unk> <unk> do nt know what love <unk> like jst take a look <unk> dis two <unk> ull <unk> it may god bless <unk> keep <unk> <unk> <unk> kachi abeg show <unk> your open in d next pix we ve <unk> it <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Original label: 2\n",
      "Predicted label: 0\n",
      "---------------------------------\n",
      "Original tweet: caleb <unk> <unk> <unk> knack fire fear <unk> <unk> go knack you till you cum blood like say <unk> <unk> <unk> <unk> school <unk> make babe <unk> tell you say you <unk> <unk> that strong in bed <unk> four solid <unk> <unk> <unk> she <unk> you <unk> <unk> <unk> pull <unk> <unk> go home <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Original label: 0\n",
      "Predicted label: 0\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the model on a sample of the test set\n",
    "for batch in pidgin_test_loader:\n",
    "    tweet = batch.tweet\n",
    "    label = batch.label\n",
    "    \n",
    "    # Print the original text and label\n",
    "    text_string = ' '.join([TEXT.vocab.itos[token.item()] for token in tweet[:, 0]])\n",
    "    label_string = LABEL.vocab.itos[label[0].item()]\n",
    "    print('Original tweet:', text_string)\n",
    "    print('Original label:', label_string)\n",
    "    \n",
    "    # Make a prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(tweet)\n",
    "        prediction = torch.argmax(output, dim=1)\n",
    "        prediction_string = LABEL.vocab.itos[prediction[0].item()]\n",
    "        print('Predicted label:', prediction_string)\n",
    "    \n",
    "    print('---------------------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
